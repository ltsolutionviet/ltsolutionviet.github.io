<head>
	<meta http-equiv="Content-Type" content="text/html;charset=UTF-8" />
	<link href="../style.css" rel="stylesheet" />
</head>
<div class="reading-text panel panel-default readingPassage style-5 panel-left panel-top interactive-panel resizable">


            <div class="text-center">
                <h2>Mind readers</h2>
                <img src="mind-readers.jpg" class="img-responsive" style="max-width: 400px; margin: auto;">
            </div>
            <p style="text-align: center;"><strong><em>It may one day be possible to eavesdrop on another person’s inner voice.&nbsp;</em></strong></p>
<div>
<p>As you begin to read this article and your eyes follow the words across the page, you may be aware of a voice in your head silently muttering along. The very same thing happens when we write: a private, internal narrative shapes the words before we commit them to text.</p>
<p>What if it were possible to tap into this inner voice? Thinking of words does, after all. create characteristic electrical signals in our brains, and decoding them could make it possible to piece together someone’s thoughts. Such an ability would have <span class="explainq2 explain">phenomenal prospects,</span> not least for people unable to communicate as a result of brain damage. But it would also carry <span class="explainq2 explain">profoundly worrisome implications</span> for the future of privacy.</p>
<p>The first scribbled records of electrical activity in the human brain were made in 1924 by a German doctor called Hans Berger using his new invention - the electroencephalogram (EEG). This uses electrodes placed on the skull to read the output of the brain's billions of nerve cells or neurons. By the mid-1990s, the ability to translate the brain's activity into readable signals <span class="explainq3 explain">had advanced</span> so far that people could move computer cursors using only the electrical fields created by their thoughts.</p>
<p>The electrical impulses such innovations tap into are produced in a part of the brain called the <span class="explainq9 explain">motor cortex, which is responsible for muscle movement</span>. To move a cursor on a screen, you do not think 'move left’ in natural language. Instead, you imagine a specific motion like hitting a ball with a tennis racket. Training the machine to realise which electrical signals correspond to your imagined movements, however, is <span class="explainq4 explain">time consuming and difficult</span>. And while this method works well for directing objects on a screen, its drawbacks become apparent when you try using it to communicate. At best, you can use the cursor to select letters displayed on an on-screen keyboard. Even a practised mind would be lucky to write 15 words per minute with that approach. Speaking, we can manage 150.</p>
<p>Matching the speed at which we can think and talk would lead to devices that could instantly translate the electrical signals of someone’s inner voice into sound produced by a speech synthesiser. To do this, it is necessary to focus only on the signals coming from the brain areas that govern speech. However, real mind reading requires some way to intercept those signals before they hit the motor cortex.</p>
<p>The translation of thoughts to language in the brain is an incredibly complex and largely mysterious process, but this much is known: before they end up in the motor cortex, <span class="explainq6 explain">thoughts destined to become spoken words pass through two ‘staging areas’ associated with the perception and expression of speech.</span></p>
<p>The first is called Wernicke’s area, which deals with <span class="explainq7 explain">semantics</span> - in this case, ideas based in meaning, which can include images, smells or emotional memories. Damage to Wernicke’s area can result in the loss of semantic associations: words can’t make sense when they are decoupled from their meaning. Suffer a stroke in that region, for example, and you will have trouble understanding not just what others are telling you, but what you yourself are thinking.</p>
The second is called Broca’s area, agreed to be <span class="explainq8 explain">the brain’s speech-processing centre</span>. Here, semantics are translated into phonetics and. ultimately, word components. From here, the assembled sentences take a quick trip to the motor cortex, which activates the muscles that will turn the desired words into speech.</div>
<div>
<p>Injure Broca’s area, and though you might know what you want to say. you just can’t send those impulses.</p>
<p>When you listen to your inner voice, two things arc happening. You ‘hear’ yourself producing language in Wernicke’s area as you construct it in Broca’s area. The key to mind reading seems to lie in these two areas.</p>
<p>44 The work of Bradley Greger in 2010 broke new ground by marking the first-ever excursion beyond the motor cortex into the brain’s language centres. His team used electrodes placed inside the skull to detect the electrical signatures of whole words, such as 'yes’, ’no’, ’hot’, ‘cold’, 'thirsty', ‘hungry’, etc. Promising as it is. this approach requires a new signal to be learned for each new word. English contains a quarter of a million distinct words. And though this was the first instance of monitoring Wernicke’s area, it still relied largely on the facial motor cortex.</p>
<p>Greger decided there might be another way. The building blocks of language are called phonemes, and the English language has about 40 of them - the ‘kuh’ sound in ‘school’, for example the ’$h' in ‘shy’. Every English word contains some subset of these components. <span class="explainq10 explain">Decode the brain signals that correspond to the phonemes, and you would have a system to unlock any word at the moment someone thinks it.</span></p>
<p>In 2011, Eric Leuthardt and his colleague Gerwin Schalk positioned electrodes over the language regions of four fully conscious people and were able to detect the phonemes ’oo’, ‘ah’, ‘eh’ and ‘ee’. What they also discovered was that spoken phonemes activated both the language areas and the motor cortex, while imagined speech - that inner voice - boosted the activity of neurons in Wernike’s area. Leuthardt had effectively read his subjects' minds. ‘I would call it brain reading,’ he says. To arrive at whole words. Leuthardt’s next step is to expand his library of sounds and to find out how the production of phonemes translates across different languages.</p>
<p>For now. the research is primarily aimed at improving the lives of people with locked-in syndrome, but the ability to explore the brain’s language centres could revolutionise other fields. The consequences of these findings could ripple out to more general audiences who might like to use extreme hands-free mobile communication technologies that can be manipulated by inner voice alone. For linguists, it could provide previously unobtainable insight into the neural origins and structures of language. Knowing what someone is thinking without needing words at all would be functionally indistinguishable from telepathy.</p>
</div>
        </div>